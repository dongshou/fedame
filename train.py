"""
FedAME ä¸»è®­ç»ƒè„šæœ¬
è”é‚¦ç±»å¢é‡å­¦ä¹ å®éªŒ
"""

import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Subset
import numpy as np
import random
import os
import json
from datetime import datetime
from typing import Dict, List, Optional

from config import get_config, Config
from data import CIFAR10Federated, create_data_loaders
from models import (
    create_backbone,
    AnchorBasedRouter,
    ExpertPool,
    DistributionPool
)
from anchor import create_anchor_generator, LLMDecisionMaker
from federated import FedAMEClient, FedAMEServer


def set_seed(seed: int):
    """è®¾ç½®éšæœºç§å­"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False


def create_clients(
    num_clients: int,
    backbone: nn.Module,
    server: FedAMEServer,
    config: Config
) -> List[FedAMEClient]:
    """åˆ›å»ºå®¢æˆ·ç«¯"""
    clients = []
    
    for k in range(num_clients):
        # åˆ›å»ºè·¯ç”±å±‚ï¼ˆä»æœåŠ¡ç«¯å¤åˆ¶åˆå§‹å‚æ•°ï¼‰
        router = AnchorBasedRouter(
            input_dim=config.model.feature_dim,
            hidden_dim=config.model.router_hidden_dim,
            anchor_dim=config.model.anchor_dim,
            temperature=config.training.temperature_route
        )
        router.load_state_dict(server.global_router.state_dict(), strict=False)
        # è®¾ç½®é”šç‚¹
        router.set_class_anchors(server.class_anchors.clone())
        router.set_cluster_anchors(
            server.cluster_anchors.clone(),
            server.global_router.cluster_to_expert
        )
        
        # åˆ›å»ºä¸“å®¶æ± ï¼ˆä»æœåŠ¡ç«¯å¤åˆ¶åˆå§‹å‚æ•°ï¼‰
        expert_pool = ExpertPool(
            input_dim=config.model.anchor_dim,
            hidden_dim=config.model.expert_hidden_dim,
            output_dim=config.model.expert_output_dim,
            num_initial_experts=server.global_expert_pool.num_experts
        )
        for exp_id in server.global_expert_pool.experts.keys():
            expert_pool.experts[exp_id].load_state_dict(
                server.global_expert_pool.experts[exp_id].state_dict()
            )
        
        # åˆ›å»ºåˆ†å¸ƒæ± 
        distribution_pool = DistributionPool(
            anchor_dim=config.model.anchor_dim
        )
        distribution_pool.set_anchors(server.class_anchors)
        
        # åˆ›å»ºå®¢æˆ·ç«¯
        client = FedAMEClient(
            client_id=k,
            backbone=backbone,
            router=router,
            expert_pool=expert_pool,
            distribution_pool=distribution_pool,
            class_anchors=server.class_anchors,
            cluster_anchors=server.cluster_anchors,
            device=config.device,
            learning_rate=config.training.learning_rate,
            weight_decay=config.training.weight_decay
        )
        
        clients.append(client)
    
    return clients


def train_task(
    task_id: int,
    task_classes: List[int],
    server: FedAMEServer,
    clients: List[FedAMEClient],
    fed_data: CIFAR10Federated,
    config: Config
) -> Dict:
    """
    è®­ç»ƒå•ä¸ªä»»åŠ¡
    
    Args:
        task_id: ä»»åŠ¡ID
        task_classes: ä»»åŠ¡ç±»åˆ«
        server: æœåŠ¡ç«¯
        clients: å®¢æˆ·ç«¯åˆ—è¡¨
        fed_data: è”é‚¦æ•°æ®é›†
        config: é…ç½®
    
    Returns:
        metrics: è®­ç»ƒæŒ‡æ ‡
    """
    print(f"\n{'='*60}")
    print(f"Task {task_id + 1}: {[fed_data.class_names[c] for c in task_classes]}")
    print('='*60)
    
    # 1. æœåŠ¡ç«¯å‡†å¤‡ä»»åŠ¡
    task_info = server.prepare_task(task_classes)
    old_classes = task_info['old_classes']
    
    # 2. è·å–å®¢æˆ·ç«¯æ•°æ®åˆ’åˆ†
    client_data = fed_data.get_client_task_data(task_classes)
    
    # 3. ä¸ºæ¯ä¸ªå®¢æˆ·ç«¯é…ç½®
    for k, (indices, local_classes) in client_data.items():
        if len(local_classes) == 0:
            continue
        
        # è·å–å®¢æˆ·ç«¯é…ç½®
        client_config = server.get_client_config(k, local_classes)
        
        # åŠ è½½å…¨å±€æ¨¡å‹åˆ°å®¢æˆ·ç«¯
        clients[k].load_global_model(
            client_config['router_state'],
            client_config['expert_states'],
            client_config['distribution_params']
        )
        
        # è®¾ç½®æœ¬åœ°æ•°æ®ä¿¡æ¯
        clients[k].setup_local_data(
            local_classes=local_classes,
            local_experts=client_config['local_experts'],
            class_to_expert=client_config['class_to_expert'],
            expert_to_cluster=client_config['expert_to_cluster']
        )
        
        # ä¿å­˜æ—§æ¨¡å‹ï¼ˆç”¨äºé˜²é—å¿˜ï¼‰
        if len(old_classes) > 0:
            clients[k].save_old_model()
    
    # 4. è”é‚¦è®­ç»ƒ
    all_metrics = []
    
    # é¢„å…ˆåˆ›å»ºæµ‹è¯•é›†
    test_classes = list(set(server.learned_classes + task_classes))
    test_dataset = fed_data.get_cumulative_test_data(test_classes)
    test_loader = create_data_loaders(
        test_dataset,
        batch_size=config.federated.local_batch_size * 2,
        shuffle=False
    )
    
    for round_idx in range(config.training.num_rounds):
        round_metrics = {'round': round_idx + 1}
        client_updates = {}
        client_distribution_params = {}
        
        # 4.1 é€‰æ‹©å‚ä¸çš„å®¢æˆ·ç«¯
        all_active_clients = []
        for k, (indices, local_classes) in client_data.items():
            if len(local_classes) > 0 and len(indices) > 0:
                all_active_clients.append(k)
        
        if len(all_active_clients) == 0:
            print(f"Round {round_idx + 1}: No active clients")
            continue
        
        # æŒ‰participation_rateéšæœºé€‰æ‹©å®¢æˆ·ç«¯
        num_selected = max(1, int(len(all_active_clients) * config.federated.participation_rate))
        active_clients = random.sample(all_active_clients, num_selected)
        
        # 4.2 å®¢æˆ·ç«¯æœ¬åœ°è®­ç»ƒ
        client_losses = []
        client_train_accs = []
        client_sample_counts = {}  # å®¢æˆ·ç«¯æ€»æ ·æœ¬æ•°
        client_class_counts = {}   # å®¢æˆ·ç«¯æ¯ç±»æ ·æœ¬æ•°
        
        for k in active_clients:
            indices = client_data[k][0]
            local_classes = client_data[k][1]
            
            # è®°å½•æ ·æœ¬æ•°é‡
            client_sample_counts[k] = len(indices)
            
            # ç»Ÿè®¡æ¯ç±»æ ·æœ¬æ•°
            train_subset = Subset(fed_data.train_dataset, indices)
            class_counts = {}
            for idx in indices:
                _, label = fed_data.train_dataset[idx]
                class_counts[label] = class_counts.get(label, 0) + 1
            client_class_counts[k] = class_counts
            
            # åˆ›å»ºæœ¬åœ°æ•°æ®åŠ è½½å™¨
            train_loader = create_data_loaders(
                train_subset,
                batch_size=config.federated.local_batch_size,
                shuffle=True
            )
            
            # æœ¬åœ°è®­ç»ƒ
            for epoch in range(config.federated.local_epochs):
                metrics = clients[k].train_epoch(train_loader, old_classes)
            
            # æ”¶é›†æ›´æ–°
            client_updates[k] = clients[k].get_model_updates()
            client_distribution_params[k] = clients[k].get_distribution_params()
            
            round_metrics[f'client_{k}_loss'] = metrics['loss']
            round_metrics[f'client_{k}_train_acc'] = metrics['accuracy']
            client_losses.append(metrics['loss'])
            client_train_accs.append(metrics['accuracy'])
        
        # 4.3 æœåŠ¡ç«¯èšåˆï¼ˆä¼ å…¥æ ·æœ¬æ•°é‡ç”¨äºåŠ æƒï¼‰
        server.aggregate(
            client_updates, 
            client_distribution_params,
            client_sample_counts=client_sample_counts,
            client_class_counts=client_class_counts
        )
        
        # 4.4 åˆ†å‘æ›´æ–°åçš„å…¨å±€æ¨¡å‹
        for k in active_clients:
            local_classes = client_data[k][1]
            client_config = server.get_client_config(k, local_classes)
            clients[k].load_global_model(
                client_config['router_state'],
                client_config['expert_states'],
                client_config['distribution_params']
            )
        
        # 4.5 è¯„ä¼°å…¨å±€æ¨¡å‹
        eval_metrics = server.evaluate(test_loader)
        round_metrics['global_test_acc'] = eval_metrics['accuracy']
        
        # è®¡ç®—å¹³å‡å€¼
        avg_loss = sum(client_losses) / len(client_losses)
        avg_train_acc = sum(client_train_accs) / len(client_train_accs)
        
        # æ‰“å°æ—¥å¿—
        print(f"Round {round_idx + 1:3d}/{config.training.num_rounds} | "
              f"Clients: {len(active_clients)}/{len(all_active_clients)} | "
              f"Loss: {avg_loss:.4f} | "
              f"Train: {avg_train_acc:.2f}% | "
              f"Test: {eval_metrics['accuracy']:.2f}%", end="")
        
        # å®šæœŸæ‰“å°è¯¦ç»†çš„æ¯ç±»å‡†ç¡®ç‡
        if (round_idx + 1) % config.log_interval == 0:
            class_accs = []
            for cls in test_classes:
                key = f'class_{cls}_acc'
                if key in eval_metrics:
                    class_accs.append(f"{fed_data.class_names[cls]}:{eval_metrics[key]:.1f}%")
            print(f"\n         Per-class: {', '.join(class_accs)}")
        else:
            print()  # æ¢è¡Œ
        
        all_metrics.append(round_metrics)
    
    # 5. å®Œæˆä»»åŠ¡
    server.finish_task(task_classes)
    
    # 6. æœ€ç»ˆè¯„ä¼°
    test_dataset = fed_data.get_cumulative_test_data(server.learned_classes)
    test_loader = create_data_loaders(
        test_dataset,
        batch_size=config.federated.local_batch_size * 2,
        shuffle=False
    )
    
    final_metrics = server.evaluate(test_loader)
    
    print(f"\n{'â”€'*60}")
    print(f"Task {task_id + 1} Completed!")
    print(f"{'â”€'*60}")
    print(f"  Overall Accuracy: {final_metrics['accuracy']:.2f}%")
    print(f"  Learned classes: {[fed_data.class_names[c] for c in server.learned_classes]}")
    
    # æ‰“å°å„ä»»åŠ¡ç±»åˆ«çš„å‡†ç¡®ç‡
    if task_id > 0:
        # è®¡ç®—æ—§ä»»åŠ¡çš„å¹³å‡å‡†ç¡®ç‡ï¼ˆæ£€æµ‹é—å¿˜ï¼‰
        old_task_accs = []
        for old_task_idx in range(task_id):
            old_task_classes = config.incremental.tasks[old_task_idx]
            task_acc = 0
            for cls in old_task_classes:
                key = f'class_{cls}_acc'
                if key in final_metrics:
                    task_acc += final_metrics[key]
            task_acc /= len(old_task_classes)
            old_task_accs.append(task_acc)
            print(f"  Task {old_task_idx + 1} Acc (old): {task_acc:.2f}%")
        
        # å½“å‰ä»»åŠ¡å‡†ç¡®ç‡
        current_task_acc = 0
        for cls in task_classes:
            key = f'class_{cls}_acc'
            if key in final_metrics:
                current_task_acc += final_metrics[key]
        current_task_acc /= len(task_classes)
        print(f"  Task {task_id + 1} Acc (new): {current_task_acc:.2f}%")
        
        # é—å¿˜æŒ‡æ ‡
        if old_task_accs:
            avg_old_acc = sum(old_task_accs) / len(old_task_accs)
            print(f"  Avg Old Tasks Acc: {avg_old_acc:.2f}%")
    
    print(f"{'â”€'*60}\n")
    
    return {
        'task_id': task_id,
        'task_classes': task_classes,
        'round_metrics': all_metrics,
        'final_metrics': final_metrics
    }


def main():
    """ä¸»å‡½æ•°"""
    # è·å–é…ç½®
    config = get_config()
    
    # è®¾ç½®è®¾å¤‡
    if torch.cuda.is_available():
        config.device = "cuda"
        print(f"Using GPU: {torch.cuda.get_device_name(0)}")
    else:
        config.device = "cpu"
        print("Using CPU")
    
    # è®¾ç½®éšæœºç§å­
    set_seed(config.seed)
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_dir = os.path.join(config.output_dir, f"run_{timestamp}")
    os.makedirs(output_dir, exist_ok=True)
    
    print("\n" + "="*60)
    print("FedAME: Federated Anchor-guided MoE")
    print("for Class-Incremental Learning")
    print("="*60)
    
    # åˆ›å»ºè”é‚¦æ•°æ®é›†
    print("\n[1] Loading CIFAR-10 dataset...")
    fed_data = CIFAR10Federated(
        data_root=config.data.data_root,
        num_clients=config.federated.num_clients,
        alpha=config.federated.alpha,
        seed=config.seed
    )
    
    # åˆ›å»ºæœåŠ¡ç«¯
    print("\n[2] Initializing server...")
    model_config = {
        'backbone': config.model.backbone,
        'backbone_pretrained': config.model.backbone_pretrained,
        'feature_dim': config.model.feature_dim,
        'router_hidden_dim': config.model.router_hidden_dim,
        'anchor_dim': config.model.anchor_dim,
        'expert_hidden_dim': config.model.expert_hidden_dim,
        'expert_output_dim': config.model.expert_output_dim,
        'temperature_route': config.training.temperature_route
    }
    
    server = FedAMEServer(
        num_classes=config.data.num_classes,
        class_names=config.data.class_names,
        cluster_config=config.data.semantic_clusters,
        model_config=model_config,
        device=config.device,
        use_clip=False,  # ä½¿ç”¨ç®€åŒ–ç‰ˆé”šç‚¹ç”Ÿæˆå™¨
        use_real_llm=config.anchor.use_real_llm
    )
    
    # åˆ›å»ºå…±äº«backbone
    print("\n[3] Creating shared backbone...")
    backbone = create_backbone(
        backbone_type=config.model.backbone,
        pretrained=config.model.backbone_pretrained,
        frozen=True
    ).to(config.device)
    
    # åˆ›å»ºå®¢æˆ·ç«¯
    print("\n[4] Creating clients...")
    clients = create_clients(
        num_clients=config.federated.num_clients,
        backbone=backbone,
        server=server,
        config=config
    )
    print(f"Created {len(clients)} clients")
    
    # è®­ç»ƒæ‰€æœ‰ä»»åŠ¡
    print("\n[5] Starting incremental learning...")
    all_results = []
    
    for task_id, task_classes in enumerate(config.incremental.tasks):
        result = train_task(
            task_id=task_id,
            task_classes=task_classes,
            server=server,
            clients=clients,
            fed_data=fed_data,
            config=config
        )
        all_results.append(result)
    
    # æœ€ç»ˆè¯„ä¼°
    print("\n" + "="*60)
    print("Final Evaluation on All Classes")
    print("="*60)
    
    test_dataset = fed_data.get_cumulative_test_data(server.learned_classes)
    test_loader = create_data_loaders(
        test_dataset,
        batch_size=config.federated.local_batch_size * 2,
        shuffle=False
    )
    
    final_metrics = server.evaluate(test_loader)
    
    print(f"\nğŸ“Š Final Results:")
    print(f"   Overall Accuracy: {final_metrics['accuracy']:.2f}%")
    print(f"   Total classes learned: {len(server.learned_classes)}")
    
    # æ‰“å°æ¯ä¸ªä»»åŠ¡çš„æœ€ç»ˆå‡†ç¡®ç‡
    print(f"\nğŸ“ˆ Per-Task Accuracy:")
    for task_idx, task_cls in enumerate(config.incremental.tasks):
        task_acc = 0
        class_details = []
        for cls in task_cls:
            key = f'class_{cls}_acc'
            if key in final_metrics:
                task_acc += final_metrics[key]
                class_details.append(f"{fed_data.class_names[cls]}:{final_metrics[key]:.1f}%")
        task_acc /= len(task_cls)
        print(f"   Task {task_idx + 1}: {task_acc:.2f}% ({', '.join(class_details)})")
    
    # æ‰“å°ä¸“å®¶ç»“æ„
    print(f"\nğŸ”§ Final Expert Structure:")
    expert_info = server.global_expert_pool.get_expert_info()
    for exp_id, info in expert_info.items():
        class_names = [fed_data.class_names[c] for c in info['responsible_classes']]
        print(f"   Expert {exp_id}: {class_names}")
    
    # ä¿å­˜ç»“æœ
    results = {
        'config': {
            'num_clients': config.federated.num_clients,
            'num_tasks': config.incremental.num_tasks,
            'alpha': config.federated.alpha,
            'num_rounds': config.training.num_rounds,
            'local_epochs': config.federated.local_epochs
        },
        'tasks': all_results,
        'final_accuracy': final_metrics['accuracy'],
        'learned_classes': server.learned_classes
    }
    
    results_path = os.path.join(output_dir, 'results.json')
    with open(results_path, 'w') as f:
        json.dump(results, f, indent=2, default=str)
    
    print(f"\nResults saved to: {results_path}")
    
    # ä¿å­˜æ¨¡å‹
    model_path = os.path.join(output_dir, 'model.pt')
    torch.save(server.get_global_model_state(), model_path)
    print(f"Model saved to: {model_path}")
    
    return results


if __name__ == "__main__":
    results = main()