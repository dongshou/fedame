"""
FedPCI è®­ç»ƒè„šæœ¬ (é‡æž„ç‰ˆ)

æž¶æž„ï¼š
- å•ä¸€åŒåˆ†æ”¯ç½‘ç»œï¼šg_common (èšåˆ) + g_ind (ä¸èšåˆ)
- ä¸¤ä¸ªåˆ†ç±»å¤´ï¼šclassifier_common (èšåˆ) + classifier_full (ä¸èšåˆ)
- å¯å­¦ä¹ åŽŸåž‹ï¼šé€‰æ‹©æ€§èšåˆ
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, Subset
import numpy as np
import random
import os
import json
from datetime import datetime
from typing import Dict, List, Optional

from config import get_config, Config
from data import CIFAR10Federated, create_data_loaders
from models.fedpci_model import FedPCIModel
from models.backbone import create_backbone
from federated.fedpci_client import FedPCIClient
from federated.fedpci_server import FedPCIServer


def set_seed(seed: int):
    """è®¾ç½®éšæœºç§å­"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False


def create_clients(
    num_clients: int,
    num_classes: int,
    backbone: nn.Module,
    server: FedPCIServer,
    config: Config
) -> List[FedPCIClient]:
    """åˆ›å»ºå®¢æˆ·ç«¯"""
    clients = []
    
    for k in range(num_clients):
        # åˆ›å»º FedPCI æ¨¡åž‹ï¼ˆä»ŽæœåŠ¡ç«¯å¤åˆ¶ï¼‰
        model = FedPCIModel(
            num_classes=num_classes,
            input_dim=config.model.feature_dim,
            hidden_dim=getattr(config.model, 'fedpci_hidden_dim', 256),
            output_dim=getattr(config.model, 'fedpci_output_dim', 128),
            num_layers=getattr(config.model, 'fedpci_num_layers', 3),
            dropout=config.model.router_dropout
        )
        
        # ä»ŽæœåŠ¡ç«¯å¤åˆ¶æ¨¡åž‹å‚æ•°
        model.load_state_dict(server.global_model.state_dict())
        
        # åˆ›å»ºå®¢æˆ·ç«¯
        client = FedPCIClient(
            client_id=k,
            num_classes=num_classes,
            backbone=backbone,
            model=model,
            device=config.device,
            learning_rate=config.training.learning_rate,
            weight_decay=config.training.weight_decay,
            lambda_local_align=getattr(config.training, 'lambda_local_align', 0.5),
            lambda_global_align=getattr(config.training, 'lambda_global_align', 0.3),
            lambda_proto_contrast=getattr(config.training, 'lambda_proto_contrast', 0.5),
            temperature=config.training.temperature_cls
        )
        
        clients.append(client)
    
    return clients


def train_task(
    task_id: int,
    task_classes: List[int],
    server: FedPCIServer,
    clients: List[FedPCIClient],
    fed_data: CIFAR10Federated,
    config: Config
) -> Dict:
    """è®­ç»ƒå•ä¸ªä»»åŠ¡"""
    print(f"\n{'='*60}")
    print(f"Task {task_id + 1}: {[fed_data.class_names[c] for c in task_classes]}")
    print('='*60)
    
    # 1. æœåŠ¡ç«¯å‡†å¤‡ä»»åŠ¡
    task_info = server.prepare_task(task_classes)
    
    # 2. èŽ·å–å®¢æˆ·ç«¯æ•°æ®åˆ’åˆ†
    client_data = fed_data.get_client_task_data(task_classes)
    
    # 3. ä¸ºæ¯ä¸ªå®¢æˆ·ç«¯é…ç½®
    global_params = server.get_global_params()
    
    for k, (indices, local_classes) in client_data.items():
        if len(local_classes) == 0:
            continue
        
        # åŠ è½½å…¨å±€å‚æ•°
        clients[k].load_global_params(global_params)
        
        # è®¾ç½®æœ¬åœ°æ•°æ®ä¿¡æ¯
        clients[k].setup_local_data(local_classes=local_classes)
    
    # 4. è”é‚¦è®­ç»ƒ
    all_metrics = []
    
    # é¢„å…ˆåˆ›å»ºæµ‹è¯•é›†
    test_classes = list(set(server.learned_classes + task_classes))
    test_dataset = fed_data.get_cumulative_test_data(test_classes)
    test_loader = create_data_loaders(
        test_dataset,
        batch_size=config.federated.local_batch_size * 2,
        shuffle=False
    )
    
    for round_idx in range(config.training.num_rounds):
        round_metrics = {'round': round_idx + 1}
        
        # 4.1 é€‰æ‹©å‚ä¸Žçš„å®¢æˆ·ç«¯
        all_active_clients = []
        for k, (indices, local_classes) in client_data.items():
            if len(local_classes) > 0 and len(indices) > 0:
                all_active_clients.append(k)
        
        if len(all_active_clients) == 0:
            print(f"Round {round_idx + 1}: No active clients")
            continue
        
        # æŒ‰ participation_rate éšæœºé€‰æ‹©å®¢æˆ·ç«¯
        num_to_select = max(1, int(config.federated.num_clients * config.federated.participation_rate))
        num_selected = min(num_to_select, len(all_active_clients))
        active_clients = random.sample(all_active_clients, num_selected)
        
        if round_idx == 0:
            print(f"   ðŸ“Š Client selection: {num_selected}/{len(all_active_clients)} active")
        
        # 4.2 å®¢æˆ·ç«¯æœ¬åœ°è®­ç»ƒ
        client_updates = []
        client_losses = []
        
        for k in active_clients:
            indices = client_data[k][0]
            local_classes = client_data[k][1]
            
            # åŠ è½½æœ€æ–°çš„å…¨å±€å‚æ•°
            global_params = server.get_global_params()
            clients[k].load_global_params(global_params)
            
            # åˆ›å»ºæ•°æ®åŠ è½½å™¨
            train_subset = Subset(fed_data.train_dataset, indices)
            train_loader = create_data_loaders(
                train_subset,
                batch_size=config.federated.local_batch_size,
                shuffle=True
            )
            
            # æœ¬åœ°è®­ç»ƒ
            metrics = clients[k].train(
                train_loader=train_loader,
                num_epochs=config.federated.local_epochs
            )
            
            client_losses.append(metrics['loss'])
            
            # æ”¶é›†æ›´æ–°
            update = clients[k].get_update_params()
            client_updates.append(update)
        
        # 4.3 æœ¬åœ°è¯„ä¼°
        local_acc_common_list = []
        local_acc_full_list = []
        local_gains = []
        
        for k in active_clients:
            indices = client_data[k][0]
            local_classes = client_data[k][1]
            
            eval_subset = Subset(fed_data.train_dataset, indices)
            eval_loader = create_data_loaders(
                eval_subset,
                batch_size=config.federated.local_batch_size,
                shuffle=False
            )
            
            local_metrics = clients[k].evaluate(eval_loader, local_classes)
            local_acc_common_list.append(local_metrics['accuracy_common'])
            local_acc_full_list.append(local_metrics['accuracy_full'])
            local_gains.append(local_metrics['personalization_gain'])
        
        avg_local_acc_common = sum(local_acc_common_list) / len(local_acc_common_list)
        avg_local_acc_full = sum(local_acc_full_list) / len(local_acc_full_list)
        avg_local_gain = sum(local_gains) / len(local_gains)
        
        round_metrics['local_acc_common'] = avg_local_acc_common
        round_metrics['local_acc_full'] = avg_local_acc_full
        round_metrics['local_gain'] = avg_local_gain
        
        # 4.4 æœåŠ¡ç«¯èšåˆ
        verbose_aggregation = ((round_idx + 1) % config.log_interval == 0)
        server.aggregate(client_updates, verbose=verbose_aggregation)
        
        # 4.5 å…¨å±€è¯„ä¼°
        eval_metrics = server.evaluate(test_loader, test_classes)
        round_metrics.update(eval_metrics)
        
        # è®¡ç®—å¹³å‡å€¼
        avg_loss = sum(client_losses) / len(client_losses) if client_losses else 0
        
        # æ‰“å°æ—¥å¿—
        print(f"Round {round_idx + 1:3d}/{config.training.num_rounds} | "
              f"Clients: {len(active_clients)} | "
              f"Loss: {avg_loss:.4f} | "
              f"Global: {eval_metrics['accuracy_common']:.2f}% | "
              f"Local: {avg_local_acc_common:.2f}%â†’{avg_local_acc_full:.2f}% | "
              f"Gain: {avg_local_gain:+.2f}%")
        
        # å®šæœŸæ‰“å°è¯¦ç»†ä¿¡æ¯
        if (round_idx + 1) % config.log_interval == 0:
            print(f"         ðŸ“Š Per-class Global Accuracy:")
            for cls in test_classes[:5]:  # åªæ‰“å°å‰5ä¸ªç±»
                key_common = f'class_{cls}_acc_common'
                if key_common in eval_metrics:
                    print(f"            {fed_data.class_names[cls]:12s}: {eval_metrics[key_common]:.1f}%")
        
        all_metrics.append(round_metrics)
    
    # 5. å®Œæˆä»»åŠ¡
    server.finish_task(task_classes)
    
    # 6. æœ€ç»ˆè¯„ä¼°
    test_dataset = fed_data.get_cumulative_test_data(server.learned_classes)
    test_loader = create_data_loaders(
        test_dataset,
        batch_size=config.federated.local_batch_size * 2,
        shuffle=False
    )
    
    final_metrics = server.evaluate(test_loader)
    
    # 7. æœ€ç»ˆæœ¬åœ°è¯„ä¼°æ±‡æ€»
    final_local_results = []
    for k, (indices, local_classes) in client_data.items():
        if len(local_classes) > 0 and len(indices) > 0:
            eval_subset = Subset(fed_data.train_dataset, indices)
            eval_loader = create_data_loaders(
                eval_subset,
                batch_size=config.federated.local_batch_size,
                shuffle=False
            )
            local_metrics = clients[k].evaluate(eval_loader, local_classes)
            final_local_results.append({
                'client_id': k,
                'acc_common': local_metrics['accuracy_common'],
                'acc_full': local_metrics['accuracy_full'],
                'gain': local_metrics['personalization_gain']
            })
    
    avg_final_gain = sum(r['gain'] for r in final_local_results) / len(final_local_results) if final_local_results else 0
    
    print(f"\n{'â”€'*60}")
    print(f"Task {task_id + 1} Completed!")
    print(f"{'â”€'*60}")
    print(f"  Global Accuracy (common): {final_metrics['accuracy_common']:.2f}%")
    print(f"  ")
    print(f"  Local Evaluation Summary ({len(final_local_results)} clients):")
    if final_local_results:
        print(f"    Avg Local AccCommon:   {sum(r['acc_common'] for r in final_local_results)/len(final_local_results):.2f}%")
        print(f"    Avg Local AccFull:     {sum(r['acc_full'] for r in final_local_results)/len(final_local_results):.2f}%")
        print(f"    Avg Personalization Gain: {avg_final_gain:+.2f}%")
    print(f"  ")
    print(f"  Learned classes: {[fed_data.class_names[c] for c in server.learned_classes]}")
    print(f"{'â”€'*60}\n")
    
    return {
        'task_id': task_id,
        'task_classes': task_classes,
        'round_metrics': all_metrics,
        'final_metrics': final_metrics
    }


def main():
    """ä¸»å‡½æ•°"""
    # èŽ·å–é…ç½®
    config = get_config()
    
    # è®¾ç½®è®¾å¤‡
    if torch.cuda.is_available():
        config.device = "cuda"
        print(f"Using GPU: {torch.cuda.get_device_name(0)}")
    else:
        config.device = "cpu"
        print("Using CPU")
    
    # è®¾ç½®éšæœºç§å­
    set_seed(config.seed)
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_dir = os.path.join(config.output_dir, f"fedpci_run_{timestamp}")
    os.makedirs(output_dir, exist_ok=True)
    
    print("\n" + "="*60)
    print("FedPCI: Federated Prototype-based Class-Incremental Learning")
    print("(Refactored Version)")
    print("="*60)
    
    # æ‰“å°å…³é”®é…ç½®
    print(f"\nðŸ“‹ Configuration:")
    print(f"   Clients: {config.federated.num_clients}")
    print(f"   Participation rate: {config.federated.participation_rate}")
    print(f"   Dirichlet Î±: {config.federated.alpha}")
    print(f"   Local epochs: {config.federated.local_epochs}")
    print(f"   Rounds per task: {config.training.num_rounds}")
    
    # åˆ›å»ºè”é‚¦æ•°æ®é›†
    print("\n[1] Loading CIFAR-10 dataset...")
    fed_data = CIFAR10Federated(
        data_root=config.data.data_root,
        num_clients=config.federated.num_clients,
        alpha=config.federated.alpha,
        seed=config.seed
    )
    
    num_classes = config.data.num_classes
    
    # åˆ›å»ºæœåŠ¡ç«¯
    print("\n[2] Initializing FedPCI server...")
    model_config = {
        'backbone': config.model.backbone,
        'backbone_pretrained': config.model.backbone_pretrained,
        'feature_dim': config.model.feature_dim,
        'hidden_dim': getattr(config.model, 'fedpci_hidden_dim', 256),
        'output_dim': getattr(config.model, 'fedpci_output_dim', 128),
        'num_layers': getattr(config.model, 'fedpci_num_layers', 3),
        'dropout': config.model.router_dropout
    }
    
    server = FedPCIServer(
        num_classes=num_classes,
        class_names=config.data.class_names,
        model_config=model_config,
        device=config.device,
        momentum=getattr(config.training, 'aggregation_momentum', 0.5)
    )
    
    # æ‰“å°æ¨¡åž‹å‚æ•°é‡
    total_params = sum(p.numel() for p in server.global_model.parameters())
    print(f"   Total model parameters: {total_params:,}")
    print(f"   Number of classes: {num_classes}")
    
    # åˆ›å»ºå…±äº« backbone
    print("\n[3] Creating shared backbone...")
    backbone = create_backbone(
        backbone_type=config.model.backbone,
        pretrained=config.model.backbone_pretrained,
        frozen=True
    ).to(config.device)
    
    # åˆ›å»ºå®¢æˆ·ç«¯
    print("\n[4] Creating clients...")
    clients = create_clients(
        num_clients=config.federated.num_clients,
        num_classes=num_classes,
        backbone=backbone,
        server=server,
        config=config
    )
    print(f"   Created {len(clients)} clients")
    
    # è®­ç»ƒæ‰€æœ‰ä»»åŠ¡
    print("\n[5] Starting incremental learning...")
    all_results = []
    
    for task_id, task_classes in enumerate(config.incremental.tasks):
        result = train_task(
            task_id=task_id,
            task_classes=task_classes,
            server=server,
            clients=clients,
            fed_data=fed_data,
            config=config
        )
        all_results.append(result)
    
    # æœ€ç»ˆè¯„ä¼°
    print("\n" + "="*60)
    print("Final Evaluation on All Classes")
    print("="*60)
    
    test_dataset = fed_data.get_cumulative_test_data(server.learned_classes)
    test_loader = create_data_loaders(
        test_dataset,
        batch_size=config.federated.local_batch_size * 2,
        shuffle=False
    )
    
    final_metrics = server.evaluate(test_loader)
    
    print(f"\nðŸ“Š Global Results:")
    print(f"   Accuracy (common): {final_metrics['accuracy_common']:.2f}%")
    print(f"   Accuracy (full):   {final_metrics['accuracy_full']:.2f}%")
    print(f"   Total classes learned: {len(server.learned_classes)}")
    
    # æœ€ç»ˆæœ¬åœ°è¯„ä¼°
    print(f"\nðŸ“Š Local Results:")
    
    all_local_results = []
    client_data = fed_data.get_client_task_data(server.learned_classes)
    
    for k, (indices, local_classes) in client_data.items():
        if len(local_classes) > 0 and len(indices) > 0:
            eval_subset = Subset(fed_data.train_dataset, indices)
            eval_loader = create_data_loaders(
                eval_subset,
                batch_size=config.federated.local_batch_size,
                shuffle=False
            )
            local_metrics = clients[k].evaluate(eval_loader, local_classes)
            all_local_results.append({
                'client_id': k,
                'num_classes': len(local_classes),
                'acc_common': local_metrics['accuracy_common'],
                'acc_full': local_metrics['accuracy_full'],
                'gain': local_metrics['personalization_gain']
            })
    
    if all_local_results:
        avg_common = sum(r['acc_common'] for r in all_local_results) / len(all_local_results)
        avg_full = sum(r['acc_full'] for r in all_local_results) / len(all_local_results)
        avg_gain = sum(r['gain'] for r in all_local_results) / len(all_local_results)
        
        print(f"   Total clients evaluated: {len(all_local_results)}")
        print(f"   Avg Local AccCommon:     {avg_common:.2f}%")
        print(f"   Avg Local AccFull:       {avg_full:.2f}%")
        print(f"   Avg Personalization Gain: {avg_gain:+.2f}%")
        
        # ç»Ÿè®¡ gain çš„åˆ†å¸ƒ
        positive_gains = [r for r in all_local_results if r['gain'] > 0]
        negative_gains = [r for r in all_local_results if r['gain'] < 0]
        
        print(f"\n   Gain Distribution:")
        print(f"     Positive (g_ind helps): {len(positive_gains)} clients ({100*len(positive_gains)/len(all_local_results):.1f}%)")
        print(f"     Negative (g_ind hurts): {len(negative_gains)} clients ({100*len(negative_gains)/len(all_local_results):.1f}%)")
    
    # æ‰“å°æ¯ç±»å…¨å±€å‡†ç¡®çŽ‡
    print(f"\nðŸ“ˆ Per-Class Global Accuracy:")
    for cls in server.learned_classes:
        key_common = f'class_{cls}_acc_common'
        if key_common in final_metrics:
            print(f"   {fed_data.class_names[cls]:12s}: {final_metrics[key_common]:.1f}%")
    
    # ä¿å­˜ç»“æžœ
    results = {
        'config': {
            'num_clients': config.federated.num_clients,
            'num_tasks': len(config.incremental.tasks),
            'alpha': config.federated.alpha,
            'num_rounds': config.training.num_rounds,
            'local_epochs': config.federated.local_epochs,
            'architecture': 'FedPCI (refactored)'
        },
        'tasks': all_results,
        'global_accuracy_common': final_metrics['accuracy_common'],
        'global_accuracy_full': final_metrics['accuracy_full'],
        'local_avg_accuracy_common': avg_common if all_local_results else 0,
        'local_avg_accuracy_full': avg_full if all_local_results else 0,
        'local_avg_gain': avg_gain if all_local_results else 0,
        'learned_classes': server.learned_classes
    }
    
    results_path = os.path.join(output_dir, 'results.json')
    with open(results_path, 'w') as f:
        json.dump(results, f, indent=2, default=str)
    
    print(f"\nResults saved to: {results_path}")
    
    # ä¿å­˜æ¨¡åž‹
    model_path = os.path.join(output_dir, 'model.pt')
    torch.save(server.get_global_model_state(), model_path)
    print(f"Model saved to: {model_path}")
    
    return results


if __name__ == "__main__":
    results = main()