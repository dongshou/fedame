"""
è”é‚¦å­¦ä¹ æœåŠ¡ç«¯æ¨¡å—
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
from typing import Dict, List, Optional, Tuple
import copy

from models import (
    create_backbone,
    AnchorBasedRouter,
    ExpertPool,
    DistributionPool,
    aggregate_distributions
)
from anchor import (
    create_anchor_generator,
    LLMDecisionMaker,
    ExpertManager
)


class FedAMEServer:
    """
    FedAMEæœåŠ¡ç«¯
    ç®¡ç†å…¨å±€æ¨¡å‹ã€é”šç‚¹ã€ä¸“å®¶åˆ†é…
    """
    
    def __init__(
        self,
        num_classes: int,
        class_names: List[str],
        cluster_config: Dict[str, List[str]],
        model_config: Dict,
        device: str = "cuda",
        use_clip: bool = False,
        use_real_llm: bool = False
    ):
        self.num_classes = num_classes
        self.class_names = class_names
        self.cluster_config = cluster_config
        self.cluster_names = list(cluster_config.keys())
        self.device = device
        
        # åˆå§‹åŒ–é”šç‚¹ç”Ÿæˆå™¨
        self.anchor_generator = create_anchor_generator(
            use_clip=use_clip,
            device=device
        )
        
        # ç”Ÿæˆé”šç‚¹
        self._generate_anchors()
        
        # åˆå§‹åŒ–å…¨å±€æ¨¡å‹ç»„ä»¶
        self._init_global_model(model_config)
        
        # åˆå§‹åŒ–LLMå†³ç­–å™¨å’Œä¸“å®¶ç®¡ç†å™¨
        self.llm_decision_maker = LLMDecisionMaker(use_real_llm=use_real_llm)
        self.expert_manager = ExpertManager(
            self.llm_decision_maker,
            class_names,
            self.cluster_names
        )
        self.expert_manager.initialize_experts(cluster_config)
        
        # åŒæ­¥ä¸“å®¶æ± çš„ç±»åˆ«åˆ†é…
        self._sync_expert_assignments()
        
        # å·²å­¦ä¹ çš„ç±»åˆ«
        self.learned_classes: List[int] = []
        
        # å®¢æˆ·ç«¯ä¿¡æ¯
        self.client_info: Dict[int, Dict] = {}
    
    def _generate_anchors(self):
        """ç”Ÿæˆå…¨å±€é”šç‚¹"""
        # ç”Ÿæˆç±»é”šç‚¹
        self.class_anchors = self.anchor_generator.generate_anchors(
            self.class_names
        ).to(self.device)
        
        # ç”Ÿæˆç°‡é”šç‚¹ï¼ˆå¤ç”¨ç±»é”šç‚¹å¦‚æœåç§°ç›¸åŒï¼‰
        self.cluster_anchors = self.anchor_generator.generate_anchors(
            self.cluster_names
        ).to(self.device)
        
        # éªŒè¯æ­£äº¤æ€§ï¼ˆå¦‚æœæ”¯æŒï¼‰
        if hasattr(self.anchor_generator, 'verify_orthogonality'):
            ortho_info = self.anchor_generator.verify_orthogonality()
            print(f"   ğŸ“ Anchor orthogonality: max_sim={ortho_info['max_similarity']:.4f}, "
                  f"mean_sim={ortho_info['mean_similarity']:.4f}, "
                  f"num_anchors={ortho_info['num_anchors']}")
    
    def _init_global_model(self, config: Dict):
        """åˆå§‹åŒ–å…¨å±€æ¨¡å‹"""
        # Backboneï¼ˆå†»ç»“ï¼‰
        self.backbone = create_backbone(
            backbone_type=config.get('backbone', 'resnet18'),
            pretrained=config.get('backbone_pretrained', True),
            frozen=True
        ).to(self.device)
        
        # è·¯ç”±å±‚ï¼ˆæ›´å¤æ‚çš„ç½‘ç»œï¼‰
        self.global_router = AnchorBasedRouter(
            input_dim=config.get('feature_dim', 512),
            hidden_dim=config.get('router_hidden_dim', 512),
            anchor_dim=config.get('anchor_dim', 512),
            temperature=config.get('temperature_route', 0.1),
            dropout=config.get('router_dropout', 0.1),
            num_layers=config.get('router_num_layers', 5),
            use_residual=config.get('router_use_residual', True)
        ).to(self.device)
        
        # è®¾ç½®é”šç‚¹
        self.global_router.set_class_anchors(self.class_anchors)
        self.global_router.set_cluster_anchors(
            self.cluster_anchors,
            {i: i for i in range(len(self.cluster_names))}  # åˆå§‹æ˜ å°„
        )
        
        # ä¸“å®¶æ± 
        self.global_expert_pool = ExpertPool(
            input_dim=config.get('anchor_dim', 512),
            hidden_dim=config.get('expert_hidden_dim', 256),
            output_dim=config.get('expert_output_dim', 512),
            num_initial_experts=len(self.cluster_names)
        ).to(self.device)
        
        # å…¨å±€åˆ†å¸ƒæ± ï¼ˆä½¿ç”¨æ”¹è¿›å‚æ•°é˜²æ­¢è¿‡æ‹Ÿåˆï¼‰
        self.global_distribution_pool = DistributionPool(
            dim=config.get('feature_dim', 512),
            init_std=config.get('distribution_init_std', 0.5),
            min_std=config.get('distribution_min_std', 0.1),
            max_std=config.get('distribution_max_std', 2.0),
            noise_scale=config.get('distribution_noise_scale', 0.1)
        )
    
    def _sync_expert_assignments(self):
        """åŒæ­¥ä¸“å®¶åˆ†é…åˆ°ä¸“å®¶æ± """
        for exp_id, info in self.expert_manager.expert_info.items():
            for cls in info['responsible_classes']:
                self.global_expert_pool.assign_class_to_expert(cls, exp_id)
    
    def prepare_task(
        self,
        task_classes: List[int]
    ) -> Dict:
        """
        å‡†å¤‡æ–°ä»»åŠ¡
        
        Args:
            task_classes: ä»»åŠ¡åŒ…å«çš„ç±»åˆ«ID
        
        Returns:
            task_info: ä»»åŠ¡ä¿¡æ¯ï¼ŒåŒ…æ‹¬ä¸“å®¶åˆ†é…ç­‰
        """
        new_classes = [c for c in task_classes if c not in self.learned_classes]
        
        # ä¸ºæ–°ç±»åˆ†é…ä¸“å®¶
        for cls in new_classes:
            class_name = self.class_names[cls]
            expert_id, cluster = self.expert_manager.assign_new_class(
                class_name,
                self.class_anchors,
                self.cluster_anchors
            )
            
            # åŒæ­¥åˆ°ä¸“å®¶æ± 
            self.global_expert_pool.assign_class_to_expert(cls, expert_id)
            
            # æ³¨æ„ï¼šä¸åœ¨è¿™é‡Œåˆå§‹åŒ–åˆ†å¸ƒï¼
            # åˆ†å¸ƒåº”è¯¥é€šè¿‡å®¢æˆ·ç«¯è®­ç»ƒåèšåˆè·å¾—ï¼Œè€Œä¸æ˜¯ç”¨ç©ºå€¼åˆå§‹åŒ–
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦æ‹†åˆ†ä¸“å®¶
        for exp_id in list(self.expert_manager.expert_info.keys()):
            split_result = self.expert_manager.check_and_split_expert(
                exp_id,
                self.class_anchors,
                max_classes=6
            )
            
            if split_result:
                # åœ¨ä¸“å®¶æ± ä¸­æ‰§è¡Œæ‹†åˆ†
                groups = []
                for new_exp_id in split_result:
                    groups.append(
                        self.expert_manager.expert_info[new_exp_id]['responsible_classes']
                    )
                
                if len(groups) > 1:
                    self.global_expert_pool.split_expert(exp_id, groups)
        
        # æ„å»ºä»»åŠ¡ä¿¡æ¯
        task_info = {
            'task_classes': task_classes,
            'new_classes': new_classes,
            'old_classes': self.learned_classes.copy(),
            'expert_assignments': self.global_expert_pool.class_to_expert.copy(),
            'expert_info': self.global_expert_pool.get_expert_info()
        }
        
        return task_info
    
    def get_expert_to_cluster(self) -> Dict[int, int]:
        """è·å–ä¸“å®¶åˆ°ç°‡çš„æ˜ å°„"""
        expert_to_cluster = {}
        for exp_id, info in self.expert_manager.expert_info.items():
            cluster_name = info.get('cluster', self.cluster_names[0])
            cluster_idx = self.cluster_names.index(cluster_name) if cluster_name in self.cluster_names else 0
            expert_to_cluster[exp_id] = cluster_idx
        return expert_to_cluster
    
    def get_client_config(
        self,
        client_id: int,
        client_classes: List[int]
    ) -> Dict:
        """
        è·å–å®¢æˆ·ç«¯é…ç½®
        
        Args:
            client_id: å®¢æˆ·ç«¯ID
            client_classes: å®¢æˆ·ç«¯æ‹¥æœ‰çš„ç±»åˆ«
        
        Returns:
            config: å®¢æˆ·ç«¯é…ç½®
        """
        # ç¡®å®šå®¢æˆ·ç«¯éœ€è¦çš„ä¸“å®¶
        needed_experts = set()
        for cls in client_classes:
            exp_id = self.global_expert_pool.get_expert_for_class(cls)
            needed_experts.add(exp_id)
        
        # ä¿å­˜å®¢æˆ·ç«¯ä¿¡æ¯
        self.client_info[client_id] = {
            'classes': client_classes,
            'experts': list(needed_experts)
        }
        
        # è·å–ç›¸å…³ä¸“å®¶çš„å‚æ•°
        expert_states = {}
        for exp_id in needed_experts:
            expert = self.global_expert_pool.get_expert(exp_id)
            expert_states[exp_id] = expert.state_dict()
        
        # è·å–æ‰€æœ‰å·²èšåˆçš„æœ‰æ•ˆåˆ†å¸ƒï¼ˆsample_count > 0ï¼Œç”¨äºä¼ªæ ·æœ¬è®­ç»ƒï¼‰
        distribution_params = {}
        for cls in self.global_distribution_pool.class_list:
            dist = self.global_distribution_pool.get_distribution(cls)
            # åªåˆ†å‘æœ‰çœŸå®æ ·æœ¬æ”¯æŒçš„åˆ†å¸ƒ
            if dist.sample_count.item() > 0:
                distribution_params[cls] = dist.get_params()
        
        return {
            'client_id': client_id,
            'local_classes': client_classes,
            'local_experts': list(needed_experts),
            'class_to_expert': self.global_expert_pool.class_to_expert.copy(),
            'expert_to_cluster': self.get_expert_to_cluster(),
            'router_state': self.global_router.state_dict(),
            'expert_states': expert_states,
            'distribution_params': distribution_params,
            'class_anchors': self.class_anchors.clone(),
            'cluster_anchors': self.cluster_anchors.clone()
        }
    
    def aggregate(
        self,
        client_updates: Dict[int, Dict],
        client_distribution_params: Dict[int, Dict[int, Dict]],
        client_sample_counts: Dict[int, int] = None,
        client_class_counts: Dict[int, Dict[int, int]] = None
    ):
        """
        èšåˆå®¢æˆ·ç«¯æ›´æ–°
        
        Args:
            client_updates: {client_id: {param_name: param_value}}
            client_distribution_params: {client_id: {class_id: params}}
            client_sample_counts: {client_id: num_samples} å®¢æˆ·ç«¯æ€»æ ·æœ¬æ•°
            client_class_counts: {client_id: {class_id: count}} å®¢æˆ·ç«¯æ¯ç±»æ ·æœ¬æ•°
        """
        # 1. èšåˆè·¯ç”±å±‚ï¼ˆæŒ‰å®¢æˆ·ç«¯æ•°æ®é‡åŠ æƒï¼‰
        self._aggregate_router(client_updates, client_sample_counts)
        
        # 2. èšåˆä¸“å®¶ï¼ˆæŒ‰å¯¹åº”ç±»çš„æ•°æ®é‡åŠ æƒï¼‰
        self._aggregate_experts(client_updates, client_class_counts)
        
        # 3. èšåˆåˆ†å¸ƒå‚æ•°
        self._aggregate_distributions(client_distribution_params)
    
    def _aggregate_router(
        self, 
        client_updates: Dict[int, Dict],
        client_sample_counts: Dict[int, int] = None
    ):
        """èšåˆè·¯ç”±å±‚ - æŒ‰æ•°æ®é‡åŠ æƒ"""
        if len(client_updates) == 0:
            return
        
        # è®¡ç®—æƒé‡
        if client_sample_counts:
            total_samples = sum(client_sample_counts.get(cid, 1) for cid in client_updates.keys())
            weights = {cid: client_sample_counts.get(cid, 1) / total_samples 
                      for cid in client_updates.keys()}
        else:
            # ç®€å•å¹³å‡
            weights = {cid: 1.0 / len(client_updates) for cid in client_updates.keys()}
        
        # æ”¶é›†è·¯ç”±å±‚å‚æ•°
        router_params = {}
        
        for client_id, updates in client_updates.items():
            client_weight = weights[client_id]
            
            for name, value in updates.items():
                if name.startswith('router.'):
                    param_name = name[7:]  # ç§»é™¤ 'router.' å‰ç¼€
                    if param_name not in router_params:
                        router_params[param_name] = torch.zeros_like(value)
                    router_params[param_name] += client_weight * value
        
        # æ›´æ–°å…¨å±€è·¯ç”±å±‚
        global_state = self.global_router.state_dict()
        for name, value in router_params.items():
            if name in global_state:
                global_state[name] = value
        
        self.global_router.load_state_dict(global_state)
    
    def _aggregate_experts(
        self, 
        client_updates: Dict[int, Dict],
        client_class_counts: Dict[int, Dict[int, int]] = None
    ):
        """èšåˆä¸“å®¶ - åªæœ‰æ‹¥æœ‰å¯¹åº”ç±»æ•°æ®çš„å®¢æˆ·ç«¯å‚ä¸ï¼ŒæŒ‰æ•°æ®é‡åŠ æƒ"""
        # æ”¶é›†æ¯ä¸ªä¸“å®¶çš„æ›´æ–°
        expert_updates: Dict[int, List[Dict]] = {}
        
        for client_id, updates in client_updates.items():
            for name, value in updates.items():
                if name.startswith('expert.'):
                    parts = name.split('.')
                    exp_id = int(parts[1])
                    param_name = '.'.join(parts[2:])
                    
                    if exp_id not in expert_updates:
                        expert_updates[exp_id] = []
                    
                    # æŸ¥æ‰¾æˆ–åˆ›å»ºè¯¥å®¢æˆ·ç«¯å¯¹è¯¥ä¸“å®¶çš„æ›´æ–°
                    found = False
                    for upd in expert_updates[exp_id]:
                        if upd['client_id'] == client_id:
                            upd['params'][param_name] = value
                            found = True
                            break
                    
                    if not found:
                        expert_updates[exp_id].append({
                            'client_id': client_id,
                            'params': {param_name: value}
                        })
        
        # èšåˆæ¯ä¸ªä¸“å®¶
        for exp_id, updates_list in expert_updates.items():
            if len(updates_list) == 0:
                continue
            
            expert = self.global_expert_pool.get_expert(exp_id)
            global_state = expert.state_dict()
            
            # è·å–è¯¥ä¸“å®¶è´Ÿè´£çš„ç±»
            responsible_classes = expert.responsible_classes
            
            # è®¡ç®—æ¯ä¸ªå®¢æˆ·ç«¯å¯¹è¯¥ä¸“å®¶çš„æƒé‡ï¼ˆåŸºäºå¯¹åº”ç±»çš„æ ·æœ¬æ•°ï¼‰
            if client_class_counts and responsible_classes:
                client_weights = {}
                total_weight = 0
                for upd in updates_list:
                    cid = upd['client_id']
                    # ç»Ÿè®¡è¯¥å®¢æˆ·ç«¯åœ¨è¯¥ä¸“å®¶è´Ÿè´£ç±»ä¸Šçš„æ ·æœ¬æ•°
                    weight = sum(
                        client_class_counts.get(cid, {}).get(cls, 0) 
                        for cls in responsible_classes
                    )
                    weight = max(weight, 1)  # è‡³å°‘ä¸º1
                    client_weights[cid] = weight
                    total_weight += weight
                
                # å½’ä¸€åŒ–
                for cid in client_weights:
                    client_weights[cid] /= total_weight
            else:
                # ç®€å•å¹³å‡
                client_weights = {upd['client_id']: 1.0 / len(updates_list) for upd in updates_list}
            
            aggregated = {}
            for upd in updates_list:
                weight = client_weights[upd['client_id']]
                for name, value in upd['params'].items():
                    if name not in aggregated:
                        aggregated[name] = torch.zeros_like(value)
                    aggregated[name] += weight * value
            
            for name, value in aggregated.items():
                if name in global_state:
                    global_state[name] = value
            
            expert.load_state_dict(global_state)
    
    def _aggregate_distributions(
        self,
        client_distribution_params: Dict[int, Dict[int, Dict]]
    ):
        """
        èšåˆåˆ†å¸ƒå‚æ•°
        å…³é”®ï¼šè®©ä¹‹å‰çš„å…¨å±€åˆ†å¸ƒä¹Ÿå‚ä¸èšåˆï¼Œé¿å…è¢«æ–°å®¢æˆ·ç«¯è¦†ç›–
        """
        if len(client_distribution_params) == 0:
            return
        
        # 1. æ”¶é›†å®¢æˆ·ç«¯ä¸Šä¼ çš„åˆ†å¸ƒ
        params_list = list(client_distribution_params.values())
        
        # 2. æŠŠå…¨å±€åˆ†å¸ƒæ± ä¸­å·²æœ‰çš„åˆ†å¸ƒä¹ŸåŠ å…¥èšåˆï¼ˆä½œä¸º"å†å²çŸ¥è¯†"ï¼‰
        global_existing_params = {}
        for cls in self.global_distribution_pool.class_list:
            dist = self.global_distribution_pool.get_distribution(cls)
            if dist.sample_count.item() > 0:
                global_existing_params[cls] = dist.get_params()
        
        if len(global_existing_params) > 0:
            params_list.append(global_existing_params)
        
        # 3. èšåˆï¼ˆåŠ æƒå¹³å‡ï¼Œæƒé‡åŸºäº sample_countï¼‰
        global_params = aggregate_distributions(
            params_list,
            dim=self.global_distribution_pool.dim
        )
        
        # 4. æ›´æ–°å…¨å±€åˆ†å¸ƒ
        for cls, params in global_params.items():
            if not self.global_distribution_pool.has_class(cls):
                self.global_distribution_pool.add_class(cls, init_mean=params['mean'])
            self.global_distribution_pool.set_class_params(cls, params)
    
    def finish_task(self, task_classes: List[int]):
        """
        å®Œæˆä»»åŠ¡ï¼Œæ›´æ–°å·²å­¦ä¹ ç±»åˆ«
        """
        for cls in task_classes:
            if cls not in self.learned_classes:
                self.learned_classes.append(cls)
    
    def diagnose_routing(
        self,
        test_loader: DataLoader,
        class_names: Optional[List[str]] = None
    ) -> Dict:
        """
        è¯Šæ–­è·¯ç”±æƒ…å†µ - åˆ†ææ¯ä¸ªç±»åˆ«è¢«è·¯ç”±åˆ°å“ªäº›ä¸“å®¶
        
        Args:
            test_loader: æµ‹è¯•æ•°æ®åŠ è½½å™¨
            class_names: ç±»åˆ«åç§°åˆ—è¡¨
        
        Returns:
            diagnosis: è·¯ç”±è¯Šæ–­ç»“æœ
        """
        self.backbone.eval()
        self.global_router.eval()
        self.global_expert_pool.eval()
        
        if class_names is None:
            class_names = [f"class_{i}" for i in range(len(self.class_anchors))]
        
        # ç»Ÿè®¡æ¯ä¸ªç±»è¢«è·¯ç”±åˆ°å“ªäº›ä¸“å®¶
        class_routing_stats = {}  # {class_id: {expert_id: count}}
        class_expected_expert = {}  # {class_id: expected_expert_id}
        class_total_samples = {}  # {class_id: total_count}
        
        with torch.no_grad():
            for images, labels in test_loader:
                images = images.to(self.device)
                labels = labels.to(self.device)
                
                # å‰å‘ä¼ æ’­
                backbone_features = self.backbone(images)
                routed_expert_ids, routing_probs, projected = self.global_router(backbone_features)
                
                # ç»Ÿè®¡è·¯ç”±ç»“æœ
                for i in range(len(labels)):
                    cls = labels[i].item()
                    routed_exp = routed_expert_ids[i].item()
                    
                    # åˆå§‹åŒ–ç»Ÿè®¡
                    if cls not in class_routing_stats:
                        class_routing_stats[cls] = {}
                        class_total_samples[cls] = 0
                        class_expected_expert[cls] = self.global_expert_pool.get_expert_for_class(cls)
                    
                    # ç»Ÿè®¡è·¯ç”±åˆ°çš„ä¸“å®¶
                    if routed_exp not in class_routing_stats[cls]:
                        class_routing_stats[cls][routed_exp] = 0
                    class_routing_stats[cls][routed_exp] += 1
                    class_total_samples[cls] += 1
        
        # è®¡ç®—æ¯ä¸ªç±»çš„è·¯ç”±å‡†ç¡®ç‡
        class_routing_accuracy = {}
        total_correct = 0
        total_samples = 0
        
        for cls in class_routing_stats:
            expected_exp = class_expected_expert[cls]
            correct = class_routing_stats[cls].get(expected_exp, 0)
            total = class_total_samples[cls]
            
            class_routing_accuracy[cls] = correct / total if total > 0 else 0
            total_correct += correct
            total_samples += total
        
        overall_accuracy = total_correct / total_samples if total_samples > 0 else 0
        
        return {
            'class_routing_stats': class_routing_stats,
            'class_expected_expert': class_expected_expert,
            'class_total_samples': class_total_samples,
            'class_routing_accuracy': class_routing_accuracy,
            'overall_accuracy': overall_accuracy,
            'total_correct': total_correct,
            'total_samples': total_samples,
            'class_names': class_names
        }
    
    def evaluate(
        self,
        test_loader,
        classes_to_eval: Optional[List[int]] = None
    ) -> Dict[str, float]:
        """
        è¯„ä¼°å…¨å±€æ¨¡å‹ï¼ˆåˆ†ç¦»è¯„ä¼°è·¯ç”±å’Œä¸“å®¶ï¼‰
        """
        self.backbone.eval()
        self.global_router.eval()
        self.global_expert_pool.eval()
        
        total_correct = 0
        total_samples = 0
        
        # è·¯ç”±è¯„ä¼°
        routing_correct = 0
        routing_samples = 0
        
        # ä¸“å®¶è¯„ä¼°ï¼ˆå‡è®¾è·¯ç”±æ­£ç¡®ï¼‰
        expert_correct_with_gt_routing = 0
        expert_samples = 0
        
        class_correct = {}
        class_total = {}
        class_routing_correct = {}
        class_routing_total = {}
        
        with torch.no_grad():
            for images, labels in test_loader:
                images = images.to(self.device)
                labels = labels.to(self.device)
                
                # å‰å‘ä¼ æ’­
                backbone_features = self.backbone(images)
                routed_expert_ids, routing_probs, projected = self.global_router(backbone_features)
                
                # ===== è¯„ä¼°è·¯ç”±å‡†ç¡®ç‡ =====
                target_experts = torch.tensor(
                    [self.global_expert_pool.get_expert_for_class(l.item()) for l in labels],
                    device=self.device
                )
                expert_to_cluster = self.get_expert_to_cluster()
                target_clusters = torch.tensor(
                    [expert_to_cluster.get(exp.item(), 0) for exp in target_experts],
                    device=self.device
                )
                routed_clusters = torch.argmax(routing_probs, dim=-1)
                
                routing_match = (routed_clusters == target_clusters)
                routing_correct += routing_match.sum().item()
                routing_samples += len(labels)
                
                # ===== è¯„ä¼°ä¸“å®¶å‡†ç¡®ç‡ï¼ˆä½¿ç”¨è·¯ç”±çš„ä¸“å®¶ï¼‰ =====
                cls_logits_routed, _ = self.global_expert_pool(
                    projected, routed_expert_ids, self.class_anchors
                )
                _, predicted_routed = cls_logits_routed.max(1)
                
                # ===== è¯„ä¼°ä¸“å®¶å‡†ç¡®ç‡ï¼ˆä½¿ç”¨ground truthä¸“å®¶ï¼‰ =====
                cls_logits_gt, _ = self.global_expert_pool(
                    projected, target_experts, self.class_anchors
                )
                _, predicted_gt = cls_logits_gt.max(1)
                
                for i in range(len(labels)):
                    label = labels[i].item()
                    
                    if classes_to_eval and label not in classes_to_eval:
                        continue
                    
                    if label not in class_total:
                        class_total[label] = 0
                        class_correct[label] = 0
                        class_routing_total[label] = 0
                        class_routing_correct[label] = 0
                    
                    class_total[label] += 1
                    class_routing_total[label] += 1
                    
                    # æ€»ä½“å‡†ç¡®ç‡ï¼ˆè·¯ç”±çš„ä¸“å®¶ï¼‰
                    if predicted_routed[i].item() == label:
                        class_correct[label] += 1
                        total_correct += 1
                    
                    # è·¯ç”±å‡†ç¡®ç‡
                    if routing_match[i].item():
                        class_routing_correct[label] += 1
                    
                    # ä¸“å®¶å‡†ç¡®ç‡ï¼ˆground truthè·¯ç”±ï¼‰
                    if predicted_gt[i].item() == label:
                        expert_correct_with_gt_routing += 1
                    
                    total_samples += 1
                    expert_samples += 1
        
        metrics = {
            'accuracy': total_correct / max(total_samples, 1) * 100,
            'routing_accuracy': routing_correct / max(routing_samples, 1) * 100,
            'expert_accuracy_with_gt_routing': expert_correct_with_gt_routing / max(expert_samples, 1) * 100,
            'total_samples': total_samples
        }
        
        for cls in class_total:
            metrics[f'class_{cls}_acc'] = (
                class_correct[cls] / class_total[cls] * 100
                if class_total[cls] > 0 else 0
            )
            metrics[f'class_{cls}_routing_acc'] = (
                class_routing_correct[cls] / class_routing_total[cls] * 100
                if class_routing_total[cls] > 0 else 0
            )
        
        return metrics
    
    def get_global_model_state(self) -> Dict:
        """è·å–å…¨å±€æ¨¡å‹çŠ¶æ€"""
        return {
            'router': self.global_router.state_dict(),
            'experts': {
                int(k): v.state_dict()
                for k, v in self.global_expert_pool.experts.items()
            },
            'distributions': self.global_distribution_pool.get_all_params(),
            'learned_classes': self.learned_classes.copy(),
            'expert_assignments': self.global_expert_pool.class_to_expert.copy()
        }
    
    def load_global_model_state(self, state: Dict):
        """åŠ è½½å…¨å±€æ¨¡å‹çŠ¶æ€"""
        self.global_router.load_state_dict(state['router'])
        
        for exp_id, exp_state in state['experts'].items():
            self.global_expert_pool.get_expert(exp_id).load_state_dict(exp_state)
        
        for cls, params in state['distributions'].items():
            if self.global_distribution_pool.has_class(cls):
                self.global_distribution_pool.set_class_params(cls, params)
        
        self.learned_classes = state['learned_classes']