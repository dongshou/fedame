"""
FedPCI è”é‚¦å­¦ä¹ æœåŠ¡ç«¯

æ ¸å¿ƒç‰¹ç‚¹ï¼š
- ç®¡ç†å…¨å±€æ¨¡å‹
- èšåˆè§„åˆ™ï¼š
  - g_common[c]: é€‰æ‹©æ€§èšåˆï¼ˆä»…æ‹¥æœ‰ç±»cçš„å®¢æˆ·ç«¯å‚ä¸ï¼‰
  - g_ind[c]: ä¸èšåˆï¼ˆå®Œå…¨æœ¬åœ°ï¼‰
  - åŸå‹ (Î¼, Ïƒ): é€‰æ‹©æ€§èšåˆ
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
from typing import Dict, List, Optional, Tuple
import copy

from models.fedpci_model import FedPCIModel
from models.backbone import create_backbone


class FedPCIServer:
    """
    FedPCI è”é‚¦æœåŠ¡ç«¯
    """
    
    def __init__(
        self,
        num_classes: int,
        class_names: List[str],
        model_config: Dict,
        device: str = "cuda",
        prototype_momentum: float = 0.9
    ):
        self.num_classes = num_classes
        self.class_names = class_names
        self.device = device
        self.prototype_momentum = prototype_momentum  # åŸå‹åŠ¨é‡ç³»æ•°
        
        # åˆå§‹åŒ–å…¨å±€æ¨¡å‹
        self._init_global_model(model_config)
        
        # å·²å­¦ä¹ çš„ç±»åˆ«
        self.learned_classes: List[int] = []
        
        # å®¢æˆ·ç«¯ä¿¡æ¯
        self.client_info: Dict[int, Dict] = {}
    
    def _init_global_model(self, config: Dict):
        """åˆå§‹åŒ–å…¨å±€æ¨¡å‹"""
        # Backboneï¼ˆå†»ç»“ï¼‰
        self.backbone = create_backbone(
            backbone_type=config.get('backbone', 'resnet18'),
            pretrained=config.get('backbone_pretrained', True),
            frozen=True
        ).to(self.device)
        
        # FedPCI æ¨¡å‹
        self.global_model = FedPCIModel(
            num_classes=self.num_classes,
            input_dim=config.get('feature_dim', 512),
            hidden_dim=config.get('hidden_dim', 256),
            output_dim=config.get('output_dim', 128),
            num_layers=config.get('num_layers', 3),
            dropout=config.get('dropout', 0.1),
            sigma_min=config.get('sigma_min', 0.1),
            sigma_max=config.get('sigma_max', 2.0),
            lambda_ind=config.get('lambda_ind', 0.5),
            temperature=config.get('temperature', 0.1)
        ).to(self.device)
    
    def prepare_task(self, task_classes: List[int]) -> Dict:
        """
        å‡†å¤‡æ–°ä»»åŠ¡
        
        Args:
            task_classes: ä»»åŠ¡åŒ…å«çš„ç±»åˆ«ID
        
        Returns:
            task_info: ä»»åŠ¡ä¿¡æ¯
        """
        new_classes = [c for c in task_classes if c not in self.learned_classes]
        
        return {
            'task_classes': task_classes,
            'new_classes': new_classes,
            'old_classes': self.learned_classes.copy()
        }
    
    def get_client_config(
        self,
        client_id: int,
        client_classes: List[int]
    ) -> Dict:
        """
        è·å–å®¢æˆ·ç«¯é…ç½®
        
        Args:
            client_id: å®¢æˆ·ç«¯ID
            client_classes: å®¢æˆ·ç«¯æ‹¥æœ‰çš„ç±»åˆ«
        
        Returns:
            config: å®¢æˆ·ç«¯é…ç½®
        """
        # ä¿å­˜å®¢æˆ·ç«¯ä¿¡æ¯
        self.client_info[client_id] = {
            'classes': client_classes
        }
        
        # è·å–æ‰€æœ‰ç±»çš„å…±æ€§åˆ†æ”¯å‚æ•°
        global_common_params = self.global_model.get_all_common_params()
        
        # è·å–æ‰€æœ‰ç±»çš„åŸå‹å‚æ•°
        global_prototype_params = self.global_model.get_all_prototype_params()
        
        return {
            'client_id': client_id,
            'local_classes': client_classes,
            'common_params': global_common_params,
            'prototype_params': global_prototype_params
        }
    
    def aggregate(
        self,
        client_common_updates: Dict[int, Dict[int, Dict[str, torch.Tensor]]],
        client_prototype_updates: Dict[int, Dict[int, Dict[str, torch.Tensor]]],
        verbose: bool = False
    ):
        """
        èšåˆå®¢æˆ·ç«¯æ›´æ–°
        
        èšåˆè§„åˆ™ï¼š
        - g_common[c]: é€‰æ‹©æ€§èšåˆï¼Œç”¨åŸå‹è·ç¦»åŠ æƒ + åŠ¨é‡
        - g_ind[c]: ä¸èšåˆï¼ˆå®¢æˆ·ç«¯æœ¬åœ°ä¿ç•™ï¼‰
        - prototype[c]: é€‰æ‹©æ€§èšåˆï¼Œæ ·æœ¬æ•°åŠ æƒ + åŠ¨é‡
        
        Args:
            client_common_updates: {client_id: {class_id: {param_name: param_value}}}
            client_prototype_updates: {client_id: {class_id: {param_name: param_value}}}
            verbose: æ˜¯å¦æ‰“å°è¯¦ç»†æ—¥å¿—
        """
        # è®°å½•èšåˆå‰çš„çŠ¶æ€
        if verbose:
            pre_state = self._get_model_state_summary()
        
        # 1. èšåˆå…±æ€§åˆ†æ”¯å‚æ•°ï¼ˆä½¿ç”¨åŸå‹è·ç¦»åŠ æƒ + åŠ¨é‡ï¼‰
        proto_info = self._aggregate_prototype_params(client_prototype_updates, verbose)

        new_global_prototypes = {}
        for class_id in range(self.num_classes):
            proto_params = self.global_model.get_prototype_params(class_id)
            new_global_prototypes[class_id] = proto_params['mean'].cpu().float()
        # 2. èšåˆåŸå‹å‚æ•°ï¼ˆæ ·æœ¬æ•°åŠ æƒ + åŠ¨é‡ï¼‰
        agg_info = self._aggregate_common_params(
            client_common_updates,
            client_prototype_updates,
            new_global_prototypes,  # ä¼ å…¥æ–°çš„å…¨å±€åŸå‹
            verbose=verbose
        )
        
        # è®°å½•èšåˆåçš„çŠ¶æ€
        if verbose:
            post_state = self._get_model_state_summary()
            self._print_aggregation_summary(pre_state, post_state, agg_info, proto_info)
    
    def _get_model_state_summary(self) -> Dict:
        """è·å–æ¨¡å‹çŠ¶æ€æ‘˜è¦"""
        state = {
            'prototypes': {},
            'g_common_norms': {}
        }
        for c in range(self.num_classes):
            # åŸå‹
            mu = self.global_model.get_prototype_mean(c)
            sigma = self.global_model.get_class_network(c).prototype.sigma
            state['prototypes'][c] = {
                'mu_norm': torch.norm(mu).item(),
                'mu_mean': mu.mean().item(),
                'sigma_mean': sigma.mean().item()
            }
            # g_common å‚æ•°èŒƒæ•°
            params = self.global_model.get_common_params(c)
            total_norm = sum(torch.norm(p).item() for p in params.values())
            state['g_common_norms'][c] = total_norm
        return state
    
    def _print_aggregation_summary(self, pre_state, post_state, agg_info, proto_info):
        """æ‰“å°èšåˆæ‘˜è¦"""
        print("\n         ğŸ“Š Aggregation Summary:")
        print("         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
        
        # g_common èšåˆä¿¡æ¯
        if agg_info:
            print("         â”‚ g_common aggregation (distance-weighted + momentum):   â”‚")
            for c, info in sorted(agg_info.items()):
                if info['num_clients'] > 0:
                    weights_str = ", ".join([f"{w:.2f}" for w in info['weights'][:3]])
                    if len(info['weights']) > 3:
                        weights_str += "..."
                    print(f"         â”‚   Class {c}: {info['num_clients']} clients, "
                          f"dists=[{', '.join([f'{d:.2f}' for d in info['distances'][:3]])}], "
                          f"weights=[{weights_str}]")
        
        # åŸå‹èšåˆä¿¡æ¯
        if proto_info:
            print("         â”‚ Prototype aggregation (sample-weighted + momentum):    â”‚")
            for c, info in sorted(proto_info.items()):
                if info['num_clients'] > 0:
                    print(f"         â”‚   Class {c}: {info['num_clients']} clients, "
                          f"Î¼_change={info['mu_change']:.4f}, "
                          f"Ïƒ_change={info['sigma_change']:.4f}")
        
        # çŠ¶æ€å˜åŒ–
        print("         â”‚ State changes:                                          â”‚")
        for c in range(min(5, self.num_classes)):  # åªæ‰“å°å‰5ä¸ªç±»
            pre_mu = pre_state['prototypes'][c]['mu_norm']
            post_mu = post_state['prototypes'][c]['mu_norm']
            pre_g = pre_state['g_common_norms'][c]
            post_g = post_state['g_common_norms'][c]
            print(f"         â”‚   Class {c}: Î¼_norm {pre_mu:.2f}â†’{post_mu:.2f}, "
                  f"g_norm {pre_g:.1f}â†’{post_g:.1f}")
        
        print("         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")
    
    def _aggregate_common_params(
        self,
        client_updates: Dict[int, Dict[int, Dict[str, torch.Tensor]]],
        client_prototype_updates: Dict[int, Dict[int, Dict[str, torch.Tensor]]],
        new_global_prototypes: Dict[int, torch.Tensor],  # â† æ–°å¢å‚æ•°
        verbose: bool = False
    ) -> Dict[int, Dict]:
        """
        èšåˆå…±æ€§åˆ†æ”¯å‚æ•°
        
        ä½¿ç”¨æ–°èšåˆçš„å…¨å±€åŸå‹è®¡ç®—è·ç¦»æƒé‡
        
        Args:
            client_updates: å®¢æˆ·ç«¯å…±æ€§å‚æ•°æ›´æ–°
            client_prototype_updates: å®¢æˆ·ç«¯åŸå‹æ›´æ–°ï¼ˆç”¨äºè·å–æœ¬åœ°åŸå‹ï¼‰
            new_global_prototypes: æ–°èšåˆçš„å…¨å±€åŸå‹ {class_id: mean_tensor}
        """
        agg_info = {}
        
        # æ”¶é›†æ¯ä¸ªç±»çš„æ›´æ–°
        class_updates: Dict[int, List[Tuple[int, Dict[str, torch.Tensor]]]] = {
            c: [] for c in range(self.num_classes)
        }
        
        for client_id, updates in client_updates.items():
            for class_id, params in updates.items():
                class_updates[class_id].append((client_id, params))
        
        # å¯¹æ¯ä¸ªç±»è¿›è¡Œèšåˆ
        for class_id in range(self.num_classes):
            updates = class_updates[class_id]
            
            agg_info[class_id] = {
                'num_clients': len(updates),
                'distances': [],
                'weights': [],
                'client_ids': []
            }
            
            if len(updates) == 0:
                continue
            
            # ========== å…³é”®ä¿®æ”¹ï¼šä½¿ç”¨æ–°çš„å…¨å±€åŸå‹ ==========
            global_proto = new_global_prototypes[class_id]
            # ================================================
            
            # è®¡ç®—æ¯ä¸ªå®¢æˆ·ç«¯çš„è·ç¦»å’Œæƒé‡
            distances = []
            client_ids = []
            for client_id, params in updates:
                client_ids.append(client_id)
                if client_id in client_prototype_updates and class_id in client_prototype_updates[client_id]:
                    local_proto = client_prototype_updates[client_id][class_id]['mean'].cpu().float()
                else:
                    local_proto = global_proto.clone()
                
                distance = torch.norm(local_proto - global_proto).item()
                distances.append(distance)
            
            # è®¡ç®— softmax æƒé‡
            distances_tensor = torch.tensor(distances)
            
            if distances_tensor.max() < 1e-8:
                weights = torch.ones(len(distances)) / len(distances)
            else:
                weights = torch.softmax(-distances_tensor, dim=0)
            
            agg_info[class_id]['distances'] = distances
            agg_info[class_id]['weights'] = weights.tolist()
            agg_info[class_id]['client_ids'] = client_ids
            
            # åŠ æƒèšåˆå‚æ•°
            aggregated_params = {}
            first_params = updates[0][1]
            
            for param_name in first_params.keys():
                param_sum = torch.zeros_like(first_params[param_name].cpu().float())
                for i, (client_id, params) in enumerate(updates):
                    param_sum += params[param_name].cpu().float() * weights[i].item()
                aggregated_params[param_name] = param_sum
            
            # åŠ¨é‡æ›´æ–°
            old_params = self.global_model.get_common_params(class_id)
            momentum = self.prototype_momentum
            for param_name in aggregated_params.keys():
                if param_name in old_params:
                    old_param = old_params[param_name].cpu().float()
                    aggregated_params[param_name] = (
                        momentum * old_param + (1 - momentum) * aggregated_params[param_name]
                    )
            
            self.global_model.set_common_params(class_id, aggregated_params)
    
        return agg_info
    def _aggregate_prototype_params(
        self,
        client_updates: Dict[int, Dict[int, Dict[str, torch.Tensor]]],
        verbose: bool = False
    ) -> Dict[int, Dict]:
        """
        èšåˆåŸå‹å‚æ•°ï¼ˆä½¿ç”¨åŠ¨é‡æ›´æ–°ï¼‰
        
        å¯¹äºæ¯ä¸ªç±» cï¼Œåªæœ‰æ‹¥æœ‰ç±» c çš„å®¢æˆ·ç«¯å‚ä¸èšåˆ
        ä½¿ç”¨åŠ¨é‡èšåˆï¼šÎ¼_new = momentum * Î¼_old + (1 - momentum) * avg(Î¼_clients)
        
        Returns:
            proto_info: æ¯ä¸ªç±»çš„åŸå‹èšåˆä¿¡æ¯
        """
        proto_info = {}
        
        # æ”¶é›†æ¯ä¸ªç±»çš„æ›´æ–°
        class_updates: Dict[int, List[Dict[str, torch.Tensor]]] = {
            c: [] for c in range(self.num_classes)
        }
        
        for client_id, updates in client_updates.items():
            for class_id, params in updates.items():
                class_updates[class_id].append(params)
        
        # å¯¹æ¯ä¸ªç±»è¿›è¡Œèšåˆ
        for class_id in range(self.num_classes):
            updates = class_updates[class_id]
            
            proto_info[class_id] = {
                'num_clients': len(updates),
                'mu_change': 0.0,
                'sigma_change': 0.0
            }
            
            if len(updates) == 0:
                continue
            
            # è·å–æ—§çš„å…¨å±€åŸå‹ï¼ˆç”¨äºè®¡ç®—å˜åŒ–ï¼‰
            old_params = self.global_model.get_prototype_params(class_id)
            old_mean = old_params['mean'].cpu().float()
            old_log_sigma = old_params['log_sigma'].cpu().float()
            
            # è®¡ç®—æ€»æ ·æœ¬æ•°ï¼ˆè½¬æ¢ä¸º float é¿å…æº¢å‡ºï¼‰
            total_count = 0.0
            for p in updates:
                if 'sample_count' in p:
                    cnt = p['sample_count']
                    if isinstance(cnt, torch.Tensor):
                        total_count += float(cnt.item())
                    else:
                        total_count += float(cnt)
                else:
                    total_count += 1.0
            
            if total_count == 0:
                total_count = float(len(updates))  # å‡ç­‰æƒé‡
            
            # èšåˆ mean å’Œ log_sigmaï¼ˆå®¢æˆ·ç«¯å¹³å‡ï¼‰
            dim = updates[0]['mean'].shape[0]
            aggregated_mean = torch.zeros(dim)
            aggregated_log_sigma = torch.zeros(dim)
            
            for params in updates:
                if 'sample_count' in params:
                    cnt = params['sample_count']
                    if isinstance(cnt, torch.Tensor):
                        count = float(cnt.item())
                    else:
                        count = float(cnt)
                else:
                    count = 1.0
                weight = count / total_count if total_count > 0 else 1.0 / len(updates)
                
                aggregated_mean += params['mean'].cpu().float() * weight
                aggregated_log_sigma += params['log_sigma'].cpu().float() * weight
            
            # åŠ¨é‡æ›´æ–°ï¼šÎ¼_new = momentum * Î¼_old + (1 - momentum) * aggregated
            momentum = self.prototype_momentum
            new_mean = momentum * old_mean + (1 - momentum) * aggregated_mean
            new_log_sigma = momentum * old_log_sigma + (1 - momentum) * aggregated_log_sigma
            
            # è®°å½•å˜åŒ–é‡
            proto_info[class_id]['mu_change'] = torch.norm(new_mean - old_mean).item()
            proto_info[class_id]['sigma_change'] = torch.norm(new_log_sigma - old_log_sigma).item()
            
            # è®¾ç½®èšåˆåçš„å‚æ•°
            self.global_model.set_prototype_params(class_id, {
                'mean': new_mean,
                'log_sigma': new_log_sigma,
                'sample_count': torch.tensor(min(total_count, 1e6))
            })
        
        return proto_info
    
    def finish_task(self, task_classes: List[int]):
        """å®Œæˆä»»åŠ¡ï¼Œæ›´æ–°å·²å­¦ä¹ ç±»åˆ«"""
        for cls in task_classes:
            if cls not in self.learned_classes:
                self.learned_classes.append(cls)
    
    def evaluate(
        self,
        test_loader: DataLoader,
        classes_to_eval: Optional[List[int]] = None
    ) -> Dict[str, float]:
        """
        è¯„ä¼°å…¨å±€æ¨¡å‹
        
        Args:
            test_loader: æµ‹è¯•æ•°æ®åŠ è½½å™¨
            classes_to_eval: è¦è¯„ä¼°çš„ç±»åˆ«
        
        Returns:
            metrics: è¯„ä¼°æŒ‡æ ‡
        """
        self.backbone.eval()
        self.global_model.eval()
        
        total_correct_common = 0
        total_correct_full = 0
        total_samples = 0
        
        class_correct_common = {}
        class_correct_full = {}
        class_total = {}
        
        with torch.no_grad():
            for images, labels in test_loader:
                images = images.to(self.device)
                labels = labels.to(self.device)
                
                # æå–ç‰¹å¾
                features = self.backbone(images)
                
                # è®¡ç®—è·ç¦»
                d_total, d_common, d_ind = self.global_model(features)
                
                # é¢„æµ‹
                pred_common = torch.argmin(d_common, dim=-1)
                pred_full = torch.argmin(d_total, dim=-1)
                
                # ç»Ÿè®¡
                for i in range(len(labels)):
                    label = labels[i].item()
                    
                    if classes_to_eval and label not in classes_to_eval:
                        continue
                    
                    if label not in class_total:
                        class_total[label] = 0
                        class_correct_common[label] = 0
                        class_correct_full[label] = 0
                    
                    class_total[label] += 1
                    total_samples += 1
                    
                    if pred_common[i].item() == label:
                        class_correct_common[label] += 1
                        total_correct_common += 1
                    
                    if pred_full[i].item() == label:
                        class_correct_full[label] += 1
                        total_correct_full += 1
        
        metrics = {
            'accuracy_common': total_correct_common / max(total_samples, 1) * 100,
            'accuracy_full': total_correct_full / max(total_samples, 1) * 100,
            'total_samples': total_samples
        }
        
        for cls in class_total:
            metrics[f'class_{cls}_acc_common'] = (
                class_correct_common[cls] / class_total[cls] * 100
                if class_total[cls] > 0 else 0
            )
            metrics[f'class_{cls}_acc_full'] = (
                class_correct_full[cls] / class_total[cls] * 100
                if class_total[cls] > 0 else 0
            )
        
        # GRPO Gain
        metrics['grpo_gain'] = metrics['accuracy_full'] - metrics['accuracy_common']
        
        return metrics
    
    def diagnose(
        self,
        test_loader: DataLoader
    ) -> Dict:
        """
        è¯Šæ–­æ¨¡å‹æ€§èƒ½
        
        åˆ†ææ¯ä¸ªç±»çš„é¢„æµ‹æƒ…å†µ
        """
        self.backbone.eval()
        self.global_model.eval()
        
        # æ··æ·†çŸ©é˜µ
        confusion_common = torch.zeros(self.num_classes, self.num_classes)
        confusion_full = torch.zeros(self.num_classes, self.num_classes)
        
        with torch.no_grad():
            for images, labels in test_loader:
                images = images.to(self.device)
                labels = labels.to(self.device)
                
                features = self.backbone(images)
                d_total, d_common, _ = self.global_model(features)
                
                pred_common = torch.argmin(d_common, dim=-1)
                pred_full = torch.argmin(d_total, dim=-1)
                
                for i in range(len(labels)):
                    true_cls = labels[i].item()
                    confusion_common[true_cls, pred_common[i].item()] += 1
                    confusion_full[true_cls, pred_full[i].item()] += 1
        
        # è®¡ç®—æ¯ç±»å‡†ç¡®ç‡
        class_acc_common = {}
        class_acc_full = {}
        
        for cls in range(self.num_classes):
            total = confusion_common[cls].sum().item()
            if total > 0:
                class_acc_common[cls] = confusion_common[cls, cls].item() / total
                class_acc_full[cls] = confusion_full[cls, cls].item() / total
            else:
                class_acc_common[cls] = 0
                class_acc_full[cls] = 0
        
        return {
            'confusion_common': confusion_common,
            'confusion_full': confusion_full,
            'class_acc_common': class_acc_common,
            'class_acc_full': class_acc_full,
            'class_names': self.class_names
        }
    
    def get_global_model_state(self) -> Dict:
        """è·å–å…¨å±€æ¨¡å‹çŠ¶æ€"""
        return {
            'model_state': self.global_model.state_dict(),
            'learned_classes': self.learned_classes.copy()
        }
    
    def load_global_model_state(self, state: Dict):
        """åŠ è½½å…¨å±€æ¨¡å‹çŠ¶æ€"""
        self.global_model.load_state_dict(state['model_state'])
        self.learned_classes = state['learned_classes']


# æµ‹è¯•
if __name__ == "__main__":
    device = "cuda" if torch.cuda.is_available() else "cpu"
    
    class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 
                   'dog', 'frog', 'horse', 'ship', 'truck']
    
    model_config = {
        'backbone': 'resnet18',
        'backbone_pretrained': True,
        'feature_dim': 512,
        'hidden_dim': 256,
        'output_dim': 128,
        'num_layers': 3,
        'dropout': 0.1,
        'sigma_min': 0.1,
        'sigma_max': 2.0,
        'lambda_ind': 0.5,
        'temperature': 0.1
    }
    
    server = FedPCIServer(
        num_classes=10,
        class_names=class_names,
        model_config=model_config,
        device=device
    )
    
    print(f"Server created on {device}")
    print(f"Number of classes: {server.num_classes}")
    
    # ç»Ÿè®¡å‚æ•°é‡
    total_params = sum(p.numel() for p in server.global_model.parameters())
    print(f"Total model parameters: {total_params:,}")
    
    # æµ‹è¯•è·å–å®¢æˆ·ç«¯é…ç½®
    config = server.get_client_config(client_id=0, client_classes=[0, 1, 2])
    print(f"\nClient config keys: {list(config.keys())}")
    print(f"Common params classes: {list(config['common_params'].keys())}")